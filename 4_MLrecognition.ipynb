{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b885fe-d050-4693-b443-32af9a2bee23",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Preparing data...\n",
      "Found 17 augmented datasets\n",
      "================================================================================\n",
      "\n",
      "Processing: Cell Proliferation (IC50)\n",
      "  Original size: 431 entries\n",
      "  Computing InChIKeys...\n",
      "  After InChIKey computation: 431 entries\n",
      "  Resolving duplicates (keeping non-augmented when both exist)...\n",
      "  After resolving duplicates: 431 entries\n",
      "  Final size: 431 unique compounds\n",
      "  Class distribution:\n",
      "    Non-augmented (AUG=False): 243 (56.4%)\n",
      "    Augmented (AUG=True):      188 (43.6%)\n",
      "  ✓ Saved to: ../data/recognizability/Cell Proliferation (IC50)_recognizability.parquet\n",
      "\n",
      "Processing: Clearance Microsomal (Mouse)\n",
      "  Original size: 4683 entries\n",
      "  Computing InChIKeys...\n",
      "  After InChIKey computation: 4683 entries\n",
      "  Resolving duplicates (keeping non-augmented when both exist)...\n",
      "  After resolving duplicates: 4683 entries\n",
      "  Final size: 4683 unique compounds\n",
      "  Class distribution:\n",
      "    Non-augmented (AUG=False): 3797 (81.1%)\n",
      "    Augmented (AUG=True):      886 (18.9%)\n",
      "  ✓ Saved to: ../data/recognizability/Clearance Microsomal (Mouse)_recognizability.parquet\n",
      "\n",
      "Processing: Clearance Microsomal (Rat)\n",
      "  Original size: 2199 entries\n",
      "  Computing InChIKeys...\n",
      "  After InChIKey computation: 2199 entries\n",
      "  Resolving duplicates (keeping non-augmented when both exist)...\n",
      "  After resolving duplicates: 2199 entries\n",
      "  Final size: 2199 unique compounds\n",
      "  Class distribution:\n",
      "    Non-augmented (AUG=False): 2149 (97.7%)\n",
      "    Augmented (AUG=True):      50 (2.3%)\n",
      "  ✓ Saved to: ../data/recognizability/Clearance Microsomal (Rat)_recognizability.parquet\n",
      "\n",
      "Processing: Clearance Renal\n",
      "  Original size: 529 entries\n",
      "  Computing InChIKeys...\n",
      "  After InChIKey computation: 529 entries\n",
      "  Resolving duplicates (keeping non-augmented when both exist)...\n",
      "  After resolving duplicates: 529 entries\n",
      "  Final size: 529 unique compounds\n",
      "  Class distribution:\n",
      "    Non-augmented (AUG=False): 529 (100.0%)\n",
      "    Augmented (AUG=True):      0 (0.0%)\n",
      "  ✓ Saved to: ../data/recognizability/Clearance Renal_recognizability.parquet\n",
      "\n",
      "Processing: Clearance Total (Rat, iv)\n",
      "  Original size: 593 entries\n",
      "  Computing InChIKeys...\n",
      "  After InChIKey computation: 593 entries\n",
      "  Resolving duplicates (keeping non-augmented when both exist)...\n",
      "  After resolving duplicates: 593 entries\n",
      "  Final size: 593 unique compounds\n",
      "  Class distribution:\n",
      "    Non-augmented (AUG=False): 592 (99.8%)\n",
      "    Augmented (AUG=True):      1 (0.2%)\n",
      "  ✓ Saved to: ../data/recognizability/Clearance Total (Rat, iv)_recognizability.parquet\n",
      "\n",
      "Processing: Efflux Ratio (Caco2)\n",
      "  Original size: 2506 entries\n",
      "  Computing InChIKeys...\n",
      "  After InChIKey computation: 2506 entries\n",
      "  Resolving duplicates (keeping non-augmented when both exist)...\n",
      "  After resolving duplicates: 2506 entries\n",
      "  Final size: 2506 unique compounds\n",
      "  Class distribution:\n",
      "    Non-augmented (AUG=False): 2471 (98.6%)\n",
      "    Augmented (AUG=True):      35 (1.4%)\n",
      "  ✓ Saved to: ../data/recognizability/Efflux Ratio (Caco2)_recognizability.parquet\n",
      "\n",
      "Processing: Efflux Ratio (MDCK-MDR1)\n",
      "  Original size: 3606 entries\n",
      "  Computing InChIKeys...\n",
      "  After InChIKey computation: 3606 entries\n",
      "  Resolving duplicates (keeping non-augmented when both exist)...\n",
      "  After resolving duplicates: 3606 entries\n",
      "  Final size: 3606 unique compounds\n",
      "  Class distribution:\n",
      "    Non-augmented (AUG=False): 3182 (88.2%)\n",
      "    Augmented (AUG=True):      424 (11.8%)\n",
      "  ✓ Saved to: ../data/recognizability/Efflux Ratio (MDCK-MDR1)_recognizability.parquet\n",
      "\n",
      "Processing: GI50 (Tumor)\n",
      "  Original size: 1933 entries\n",
      "  Computing InChIKeys...\n",
      "  After InChIKey computation: 1933 entries\n",
      "  Resolving duplicates (keeping non-augmented when both exist)...\n",
      "  After resolving duplicates: 1933 entries\n",
      "  Final size: 1933 unique compounds\n",
      "  Class distribution:\n",
      "    Non-augmented (AUG=False): 1902 (98.4%)\n",
      "    Augmented (AUG=True):      31 (1.6%)\n",
      "  ✓ Saved to: ../data/recognizability/GI50 (Tumor)_recognizability.parquet\n",
      "\n",
      "Processing: HFE\n",
      "  Original size: 455 entries\n",
      "  Computing InChIKeys...\n",
      "  After InChIKey computation: 455 entries\n",
      "  Resolving duplicates (keeping non-augmented when both exist)...\n",
      "  After resolving duplicates: 455 entries\n",
      "  Final size: 455 unique compounds\n",
      "  Class distribution:\n",
      "    Non-augmented (AUG=False): 172 (37.8%)\n",
      "    Augmented (AUG=True):      283 (62.2%)\n",
      "  ✓ Saved to: ../data/recognizability/HFE_recognizability.parquet\n",
      "\n",
      "Processing: Half Life (Human, Microsome)\n",
      "  Original size: 3645 entries\n",
      "  Computing InChIKeys...\n",
      "  After InChIKey computation: 3645 entries\n",
      "  Resolving duplicates (keeping non-augmented when both exist)...\n",
      "  After resolving duplicates: 3645 entries\n",
      "  Final size: 3645 unique compounds\n",
      "  Class distribution:\n",
      "    Non-augmented (AUG=False): 2798 (76.8%)\n",
      "    Augmented (AUG=True):      847 (23.2%)\n",
      "  ✓ Saved to: ../data/recognizability/Half Life (Human, Microsome)_recognizability.parquet\n",
      "\n",
      "Processing: Half Life (Human, Plasma)\n",
      "  Original size: 4278 entries\n",
      "  Computing InChIKeys...\n",
      "  After InChIKey computation: 4278 entries\n",
      "  Resolving duplicates (keeping non-augmented when both exist)...\n",
      "  After resolving duplicates: 4278 entries\n",
      "  Final size: 4278 unique compounds\n",
      "  Class distribution:\n",
      "    Non-augmented (AUG=False): 3630 (84.9%)\n",
      "    Augmented (AUG=True):      648 (15.1%)\n",
      "  ✓ Saved to: ../data/recognizability/Half Life (Human, Plasma)_recognizability.parquet\n",
      "\n",
      "Processing: Half Life (Rat, Microsome)\n",
      "  Original size: 1406 entries\n",
      "  Computing InChIKeys...\n",
      "  After InChIKey computation: 1406 entries\n",
      "  Resolving duplicates (keeping non-augmented when both exist)...\n",
      "  After resolving duplicates: 1406 entries\n",
      "  Final size: 1406 unique compounds\n",
      "  Class distribution:\n",
      "    Non-augmented (AUG=False): 1201 (85.4%)\n",
      "    Augmented (AUG=True):      205 (14.6%)\n",
      "  ✓ Saved to: ../data/recognizability/Half Life (Rat, Microsome)_recognizability.parquet\n",
      "\n",
      "Processing: Half Life (Rat, Plasma)\n",
      "  Original size: 1148 entries\n",
      "  Computing InChIKeys...\n",
      "  After InChIKey computation: 1148 entries\n",
      "  Resolving duplicates (keeping non-augmented when both exist)...\n",
      "  After resolving duplicates: 1148 entries\n",
      "  Final size: 1148 unique compounds\n",
      "  Class distribution:\n",
      "    Non-augmented (AUG=False): 1076 (93.7%)\n",
      "    Augmented (AUG=True):      72 (6.3%)\n",
      "  ✓ Saved to: ../data/recognizability/Half Life (Rat, Plasma)_recognizability.parquet\n",
      "\n",
      "Processing: Stability Microsomal (Mouse)\n",
      "  Original size: 4683 entries\n",
      "  Computing InChIKeys...\n",
      "  After InChIKey computation: 4683 entries\n",
      "  Resolving duplicates (keeping non-augmented when both exist)...\n",
      "  After resolving duplicates: 4683 entries\n",
      "  Final size: 4683 unique compounds\n",
      "  Class distribution:\n",
      "    Non-augmented (AUG=False): 3797 (81.1%)\n",
      "    Augmented (AUG=True):      886 (18.9%)\n",
      "  ✓ Saved to: ../data/recognizability/Stability Microsomal (Mouse)_recognizability.parquet\n",
      "\n",
      "Processing: Stability Microsomal (Rat)\n",
      "  Original size: 2199 entries\n",
      "  Computing InChIKeys...\n",
      "  After InChIKey computation: 2199 entries\n",
      "  Resolving duplicates (keeping non-augmented when both exist)...\n",
      "  After resolving duplicates: 2199 entries\n",
      "  Final size: 2199 unique compounds\n",
      "  Class distribution:\n",
      "    Non-augmented (AUG=False): 2149 (97.7%)\n",
      "    Augmented (AUG=True):      50 (2.3%)\n",
      "  ✓ Saved to: ../data/recognizability/Stability Microsomal (Rat)_recognizability.parquet\n",
      "\n",
      "Processing: TD50 (Rat)\n",
      "  Original size: 696 entries\n",
      "  Computing InChIKeys...\n",
      "  After InChIKey computation: 696 entries\n",
      "  Resolving duplicates (keeping non-augmented when both exist)...\n",
      "  After resolving duplicates: 696 entries\n",
      "  Final size: 696 unique compounds\n",
      "  Class distribution:\n",
      "    Non-augmented (AUG=False): 400 (57.5%)\n",
      "    Augmented (AUG=True):      296 (42.5%)\n",
      "  ✓ Saved to: ../data/recognizability/TD50 (Rat)_recognizability.parquet\n",
      "\n",
      "Processing: VDss (Dog)\n",
      "  Original size: 1343 entries\n",
      "  Computing InChIKeys...\n",
      "  After InChIKey computation: 1343 entries\n",
      "  Resolving duplicates (keeping non-augmented when both exist)...\n",
      "  After resolving duplicates: 1343 entries\n",
      "  Final size: 1343 unique compounds\n",
      "  Class distribution:\n",
      "    Non-augmented (AUG=False): 1343 (100.0%)\n",
      "    Augmented (AUG=True):      0 (0.0%)\n",
      "  ✓ Saved to: ../data/recognizability/VDss (Dog)_recognizability.parquet\n",
      "\n",
      "================================================================================\n",
      "Data preparation complete!\n",
      "Cleaned datasets saved in: ../data/recognizability\n",
      "\n",
      "Step 2: Running classification models on combined dataset...\n",
      "Found 17 datasets\n",
      "================================================================================\n",
      "\n",
      "Loading and combining all datasets...\n",
      "  Loaded Cell Proliferation (IC50): 431 compounds\n",
      "  Loaded Clearance Microsomal (Mouse): 4683 compounds\n",
      "  Loaded Clearance Microsomal (Rat): 2199 compounds\n",
      "  Loaded Clearance Renal: 529 compounds\n",
      "  Loaded Clearance Total (Rat, iv): 593 compounds\n",
      "  Loaded Efflux Ratio (Caco2): 2506 compounds\n",
      "  Loaded Efflux Ratio (MDCK-MDR1): 3606 compounds\n",
      "  Loaded GI50 (Tumor): 1933 compounds\n",
      "  Loaded HFE: 455 compounds\n",
      "  Loaded Half Life (Human, Microsome): 3645 compounds\n",
      "  Loaded Half Life (Human, Plasma): 4278 compounds\n",
      "  Loaded Half Life (Rat, Microsome): 1406 compounds\n",
      "  Loaded Half Life (Rat, Plasma): 1148 compounds\n",
      "  Loaded Stability Microsomal (Mouse): 4683 compounds\n",
      "  Loaded Stability Microsomal (Rat): 2199 compounds\n",
      "  Loaded TD50 (Rat): 696 compounds\n",
      "  Loaded VDss (Dog): 1343 compounds\n",
      "\n",
      "================================================================================\n",
      "COMBINED DATASET STATISTICS\n",
      "================================================================================\n",
      "Total compounds: 36333\n",
      "\n",
      "Checking for duplicates across datasets...\n",
      "  Before: 36333 compounds\n",
      "  After:  24148 compounds\n",
      "  Removed: 12185 duplicates\n",
      "\n",
      "Class distribution:\n",
      "  Non-augmented (AUG=False): 20410 (84.5%)\n",
      "  Augmented (AUG=True):      3738 (15.5%)\n",
      "\n",
      "================================================================================\n",
      "TRAINING CLASSIFICATION MODELS ON COMBINED DATASET\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FOLD 1/5\n",
      "================================================================================\n",
      "Train: 19318 compounds (Aug=2990, 15.5%)\n",
      "Test:  4830 compounds (Aug=748, 15.5%)\n",
      "\n",
      "--- Descriptor: AVALON ---\n",
      "  Generating features...\n",
      "  Train features: (19318, 2048)\n",
      "  Test features:  (4830, 2048)\n",
      "  Training replicate 1/3... BalAcc=0.777, MCC=0.623\n",
      "  Training replicate 2/3... BalAcc=0.777, MCC=0.623\n",
      "  Training replicate 3/3... BalAcc=0.777, MCC=0.623\n",
      "\n",
      "================================================================================\n",
      "FOLD 2/5\n",
      "================================================================================\n",
      "Train: 19318 compounds (Aug=2990, 15.5%)\n",
      "Test:  4830 compounds (Aug=748, 15.5%)\n",
      "\n",
      "--- Descriptor: AVALON ---\n",
      "  Generating features...\n",
      "  Train features: (19318, 2048)\n",
      "  Test features:  (4830, 2048)\n",
      "  Training replicate 1/3... BalAcc=0.770, MCC=0.619\n",
      "  Training replicate 2/3... BalAcc=0.770, MCC=0.619\n",
      "  Training replicate 3/3... BalAcc=0.770, MCC=0.619\n",
      "\n",
      "================================================================================\n",
      "FOLD 3/5\n",
      "================================================================================\n",
      "Train: 19318 compounds (Aug=2990, 15.5%)\n",
      "Test:  4830 compounds (Aug=748, 15.5%)\n",
      "\n",
      "--- Descriptor: AVALON ---\n",
      "  Generating features...\n",
      "  Train features: (19318, 2048)\n",
      "  Test features:  (4830, 2048)\n",
      "  Training replicate 1/3... BalAcc=0.777, MCC=0.621\n",
      "  Training replicate 2/3... BalAcc=0.777, MCC=0.621\n",
      "  Training replicate 3/3... BalAcc=0.777, MCC=0.621\n",
      "\n",
      "================================================================================\n",
      "FOLD 4/5\n",
      "================================================================================\n",
      "Train: 19319 compounds (Aug=2991, 15.5%)\n",
      "Test:  4829 compounds (Aug=747, 15.5%)\n",
      "\n",
      "--- Descriptor: AVALON ---\n",
      "  Generating features...\n",
      "  Train features: (19319, 2048)\n",
      "  Test features:  (4829, 2048)\n",
      "  Training replicate 1/3... BalAcc=0.775, MCC=0.612\n",
      "  Training replicate 2/3... BalAcc=0.775, MCC=0.612\n",
      "  Training replicate 3/3... BalAcc=0.775, MCC=0.612\n",
      "\n",
      "================================================================================\n",
      "FOLD 5/5\n",
      "================================================================================\n",
      "Train: 19319 compounds (Aug=2991, 15.5%)\n",
      "Test:  4829 compounds (Aug=747, 15.5%)\n",
      "\n",
      "--- Descriptor: AVALON ---\n",
      "  Generating features...\n",
      "  Train features: (19319, 2048)\n",
      "  Test features:  (4829, 2048)\n",
      "  Training replicate 1/3... BalAcc=0.777, MCC=0.618\n",
      "  Training replicate 2/3... BalAcc=0.777, MCC=0.618\n",
      "  Training replicate 3/3... BalAcc=0.777, MCC=0.618\n",
      "\n",
      "================================================================================\n",
      "AGGREGATE PERFORMANCE\n",
      "================================================================================\n",
      "\n",
      "Overall Performance (Combined Model):\n",
      "  Balanced Accuracy: 0.7753\n",
      "  MCC:               0.6187\n",
      "  Precision:         0.7685\n",
      "  Recall:            0.5827\n",
      "  F1-Score:          0.6628\n",
      "  ROC-AUC:           0.9454\n",
      "  Avg Prec:          0.7682\n",
      "\n",
      "Performance by Dataset Source:\n",
      "Dataset                                  N        BalAcc   MCC      F1       AUC     \n",
      "------------------------------------------------------------------------------------------\n",
      "GI50 (Tumor)                             1920     0.498    -0.009   0.000    0.897   \n",
      "Half Life (Human, Microsome)             3484     0.801    0.630    0.712    0.935   \n",
      "VDss (Dog)                               929      1.000    0.000    0.000    nan     \n",
      "Clearance Microsomal (Mouse)             4682     0.834    0.709    0.757    0.959   \n",
      "Efflux Ratio (MDCK-MDR1)                 2781     0.686    0.503    0.522    0.926   \n",
      "Clearance Microsomal (Rat)               1636     0.732    0.577    0.571    0.922   \n",
      "Half Life (Rat, Plasma)                  428      0.629    0.449    0.406    0.944   \n",
      "Clearance Renal                          508      0.980    0.000    0.000    nan     \n",
      "Half Life (Human, Plasma)                2845     0.698    0.490    0.543    0.904   \n",
      "Efflux Ratio (Caco2)                     2202     0.510    0.027    0.037    0.915   \n",
      "Clearance Total (Rat, iv)                535      0.499    -0.002   0.000    0.672   \n",
      "HFE                                      448      0.728    0.451    0.791    0.808   \n",
      "Half Life (Rat, Microsome)               678      0.730    0.583    0.613    0.928   \n",
      "TD50 (Rat)                               641      0.644    0.323    0.534    0.770   \n",
      "Cell Proliferation (IC50)                431      0.811    0.618    0.790    0.908   \n",
      "\n",
      "✓ Results saved to: ../data/recognizability/combined_recognizability_predictions.parquet\n",
      "  Total compounds: 24148\n",
      "  Total columns: 21\n",
      "  Prediction columns: 15\n",
      "\n",
      "Step 3: Generating summary report...\n",
      "\n",
      "================================================================================\n",
      "RECOGNIZABILITY PERFORMANCE SUMMARY (COMBINED MODEL)\n",
      "================================================================================\n",
      "\n",
      "OVERALL PERFORMANCE:\n",
      "  Total Compounds:    24148\n",
      "  Augmented:          3738 (15.5%)\n",
      "  Non-augmented:      20410 (84.5%)\n",
      "\n",
      "  Balanced Accuracy:  0.7753\n",
      "  MCC:                0.6187\n",
      "  Precision:          0.7685\n",
      "  Recall:             0.5827\n",
      "  F1-Score:           0.6628\n",
      "  ROC-AUC:            0.9454\n",
      "  Average Precision:  0.7682\n",
      "\n",
      "================================================================================\n",
      "PERFORMANCE BY DATASET SOURCE\n",
      "================================================================================\n",
      "\n",
      "Dataset                                  N        %Aug     BalAcc   MCC      F1       AUC     \n",
      "------------------------------------------------------------------------------------------\n",
      "Clearance Microsomal (Mouse)             4682     18.9     0.834    0.709    0.757    0.959   \n",
      "Half Life (Rat, Plasma)                  428      12.4     0.629    0.449    0.406    0.944   \n",
      "Half Life (Human, Microsome)             3484     24.2     0.801    0.630    0.712    0.935   \n",
      "Half Life (Rat, Microsome)               678      18.7     0.730    0.583    0.613    0.928   \n",
      "Efflux Ratio (MDCK-MDR1)                 2781     15.1     0.686    0.503    0.522    0.926   \n",
      "Clearance Microsomal (Rat)               1636     2.9      0.732    0.577    0.571    0.922   \n",
      "Efflux Ratio (Caco2)                     2202     1.6      0.510    0.027    0.037    0.915   \n",
      "Cell Proliferation (IC50)                431      43.6     0.811    0.618    0.790    0.908   \n",
      "Half Life (Human, Plasma)                2845     18.8     0.698    0.490    0.543    0.904   \n",
      "GI50 (Tumor)                             1920     1.6      0.498    -0.009   0.000    0.897   \n",
      "HFE                                      448      63.2     0.728    0.451    0.791    0.808   \n",
      "TD50 (Rat)                               641      45.2     0.644    0.323    0.534    0.770   \n",
      "Clearance Total (Rat, iv)                535      0.2      0.499    -0.002   0.000    0.672   \n",
      "Clearance Renal                          508      0.0      0.980    0.000    0.000    nan     \n",
      "VDss (Dog)                               929      0.0      1.000    0.000    0.000    nan     \n",
      "\n",
      "✓ Summary saved to: ../data/recognizability/recognizability_combined_summary.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, balanced_accuracy_score,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "import xgboost as xgb\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors, inchi\n",
    "from rdkit.Avalon import pyAvalonTools\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# Feature Generation Functions\n",
    "# ============================================================================\n",
    "\n",
    "def generate_avalon_features(smiles_list, n_bits=2048):\n",
    "    \"\"\"Generate Avalon fingerprints\"\"\"\n",
    "    features = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for idx, smiles in enumerate(smiles_list):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is not None:\n",
    "            fp = pyAvalonTools.GetAvalonFP(mol, nBits=n_bits)\n",
    "            features.append(np.array(fp))\n",
    "            valid_indices.append(idx)\n",
    "    \n",
    "    return np.array(features), valid_indices\n",
    "\n",
    "def generate_rdkit2d_features(smiles_list):\n",
    "    \"\"\"Generate RDKit 2D descriptors\"\"\"\n",
    "    features = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    descriptor_names = [desc[0] for desc in Descriptors._descList]\n",
    "    \n",
    "    for idx, smiles in enumerate(smiles_list):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is not None:\n",
    "            desc_values = []\n",
    "            for desc_name in descriptor_names:\n",
    "                desc_func = getattr(Descriptors, desc_name)\n",
    "                try:\n",
    "                    value = desc_func(mol)\n",
    "                    desc_values.append(value if not np.isnan(value) else 0)\n",
    "                except:\n",
    "                    desc_values.append(0)\n",
    "            features.append(desc_values)\n",
    "            valid_indices.append(idx)\n",
    "    \n",
    "    return np.array(features), valid_indices\n",
    "\n",
    "def generate_ecfp6_features(smiles_list, radius=3, n_bits=2048):\n",
    "    \"\"\"Generate ECFP6 (Morgan) fingerprints with radius 3\"\"\"\n",
    "    features = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for idx, smiles in enumerate(smiles_list):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is not None:\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=n_bits)\n",
    "            features.append(np.array(fp))\n",
    "            valid_indices.append(idx)\n",
    "    \n",
    "    return np.array(features), valid_indices\n",
    "\n",
    "def smiles_to_inchikey(smiles):\n",
    "    \"\"\"Convert SMILES to InChIKey\"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        return inchi.MolToInchiKey(mol)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# Data Preparation\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_recognizability_data(\n",
    "    augmented_data_dir='../data/augment_all',\n",
    "    output_dir='../data/recognizability'\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare data for recognizability analysis\n",
    "    \n",
    "    Steps:\n",
    "    1. Load all augmented datasets\n",
    "    2. Extract SMILES and AUG columns\n",
    "    3. Compute InChIKey\n",
    "    4. If compound has both AUG=True and AUG=False, keep only AUG=False\n",
    "    5. Drop duplicates\n",
    "    6. Save cleaned datasets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    augmented_data_dir : str\n",
    "        Directory containing augmented datasets\n",
    "    output_dir : str\n",
    "        Directory to save cleaned datasets\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary mapping dataset names to cleaned dataframes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get all augmented dataset files\n",
    "    dataset_files = sorted(Path(augmented_data_dir).glob('*_augmented.parquet'))\n",
    "    \n",
    "    print(f\"Found {len(dataset_files)} augmented datasets\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    cleaned_datasets = {}\n",
    "    \n",
    "    for dataset_file in dataset_files:\n",
    "        # Extract original dataset name\n",
    "        dataset_name = dataset_file.stem.replace('_augmented', '')\n",
    "        \n",
    "        print(f\"\\nProcessing: {dataset_name}\")\n",
    "        \n",
    "        # Load augmented dataset\n",
    "        df = pd.read_parquet(dataset_file)\n",
    "        \n",
    "        print(f\"  Original size: {len(df)} entries\")\n",
    "        \n",
    "        # Extract SMILES and AUG columns\n",
    "        df_clean = df[['SMILES', 'AUG']].copy()\n",
    "        \n",
    "        # Remove any NaN SMILES\n",
    "        df_clean = df_clean.dropna(subset=['SMILES'])\n",
    "        \n",
    "        # Compute InChIKey\n",
    "        print(\"  Computing InChIKeys...\")\n",
    "        df_clean['InChIKey'] = df_clean['SMILES'].apply(smiles_to_inchikey)\n",
    "        \n",
    "        # Remove any failed InChIKey conversions\n",
    "        df_clean = df_clean.dropna(subset=['InChIKey'])\n",
    "        \n",
    "        print(f\"  After InChIKey computation: {len(df_clean)} entries\")\n",
    "        \n",
    "        # For each InChIKey, if both AUG=True and AUG=False exist, keep only AUG=False\n",
    "        print(\"  Resolving duplicates (keeping non-augmented when both exist)...\")\n",
    "        \n",
    "        # Group by InChIKey and check for conflicts\n",
    "        def resolve_augmentation(group):\n",
    "            \"\"\"If both True and False exist, keep False; otherwise keep what exists\"\"\"\n",
    "            if len(group) == 1:\n",
    "                return group\n",
    "            # Check if both True and False exist\n",
    "            has_false = (group['AUG'] == False).any()\n",
    "            if has_false:\n",
    "                # Keep only non-augmented (AUG=False)\n",
    "                return group[group['AUG'] == False].iloc[[0]]\n",
    "            else:\n",
    "                # All are augmented, keep first\n",
    "                return group.iloc[[0]]\n",
    "        \n",
    "        df_resolved = df_clean.groupby('InChIKey', group_keys=False).apply(resolve_augmentation)\n",
    "        \n",
    "        print(f\"  After resolving duplicates: {len(df_resolved)} entries\")\n",
    "        \n",
    "        # Drop any remaining duplicates (shouldn't be any, but just in case)\n",
    "        df_resolved = df_resolved.drop_duplicates(subset=['InChIKey'])\n",
    "        \n",
    "        print(f\"  Final size: {len(df_resolved)} unique compounds\")\n",
    "        \n",
    "        # Count class distribution\n",
    "        n_aug = (df_resolved['AUG'] == True).sum()\n",
    "        n_non_aug = (df_resolved['AUG'] == False).sum()\n",
    "        \n",
    "        print(f\"  Class distribution:\")\n",
    "        print(f\"    Non-augmented (AUG=False): {n_non_aug} ({100*n_non_aug/len(df_resolved):.1f}%)\")\n",
    "        print(f\"    Augmented (AUG=True):      {n_aug} ({100*n_aug/len(df_resolved):.1f}%)\")\n",
    "        \n",
    "        # Add dataset source column\n",
    "        df_resolved['dataset_source'] = dataset_name\n",
    "        \n",
    "        # Save cleaned dataset\n",
    "        output_file = Path(output_dir) / f'{dataset_name}_recognizability.parquet'\n",
    "        df_resolved.to_parquet(output_file, index=False)\n",
    "        \n",
    "        print(f\"  ✓ Saved to: {output_file}\")\n",
    "        \n",
    "        cleaned_datasets[dataset_name] = df_resolved\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Data preparation complete!\")\n",
    "    print(f\"Cleaned datasets saved in: {output_dir}\")\n",
    "    \n",
    "    return cleaned_datasets\n",
    "\n",
    "# ============================================================================\n",
    "# Recognizability Classification - COMBINED DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "def run_recognizability_classification_combined(\n",
    "    data_dir='../data/recognizability',\n",
    "    output_dir='../data/recognizability',\n",
    "    n_folds=5,\n",
    "    n_replicates=3,\n",
    "    random_seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Run classification models on ALL datasets combined\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dir : str\n",
    "        Directory containing cleaned recognizability datasets\n",
    "    output_dir : str\n",
    "        Directory to save prediction results\n",
    "    n_folds : int\n",
    "        Number of cross-validation folds\n",
    "    n_replicates : int\n",
    "        Number of model replicates\n",
    "    random_seed : int\n",
    "        Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get all dataset files\n",
    "    dataset_files = sorted(Path(data_dir).glob('*_recognizability.parquet'))\n",
    "    \n",
    "    print(f\"Found {len(dataset_files)} datasets\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load and combine all datasets\n",
    "    print(\"\\nLoading and combining all datasets...\")\n",
    "    all_dfs = []\n",
    "    \n",
    "    for dataset_file in dataset_files:\n",
    "        dataset_name = dataset_file.stem.replace('_recognizability', '')\n",
    "        df = pd.read_parquet(dataset_file)\n",
    "        \n",
    "        # Ensure dataset_source column exists\n",
    "        if 'dataset_source' not in df.columns:\n",
    "            df['dataset_source'] = dataset_name\n",
    "        \n",
    "        all_dfs.append(df)\n",
    "        print(f\"  Loaded {dataset_name}: {len(df)} compounds\")\n",
    "    \n",
    "    # Combine all datasets\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"COMBINED DATASET STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total compounds: {len(combined_df)}\")\n",
    "    \n",
    "    # Check for global duplicates across datasets (by InChIKey)\n",
    "    print(f\"\\nChecking for duplicates across datasets...\")\n",
    "    n_before = len(combined_df)\n",
    "    \n",
    "    # If compound appears in multiple datasets, keep only one (prioritize non-augmented)\n",
    "    def resolve_global_duplicates(group):\n",
    "        if len(group) == 1:\n",
    "            return group\n",
    "        # Prioritize non-augmented\n",
    "        has_false = (group['AUG'] == False).any()\n",
    "        if has_false:\n",
    "            return group[group['AUG'] == False].iloc[[0]]\n",
    "        else:\n",
    "            return group.iloc[[0]]\n",
    "    \n",
    "    combined_df = combined_df.groupby('InChIKey', group_keys=False).apply(resolve_global_duplicates)\n",
    "    combined_df = combined_df.reset_index(drop=True)\n",
    "    \n",
    "    n_after = len(combined_df)\n",
    "    print(f\"  Before: {n_before} compounds\")\n",
    "    print(f\"  After:  {n_after} compounds\")\n",
    "    print(f\"  Removed: {n_before - n_after} duplicates\")\n",
    "    \n",
    "    # Class distribution\n",
    "    n_aug = (combined_df['AUG'] == True).sum()\n",
    "    n_non_aug = (combined_df['AUG'] == False).sum()\n",
    "    \n",
    "    print(f\"\\nClass distribution:\")\n",
    "    print(f\"  Non-augmented (AUG=False): {n_non_aug} ({100*n_non_aug/len(combined_df):.1f}%)\")\n",
    "    print(f\"  Augmented (AUG=True):      {n_aug} ({100*n_aug/len(combined_df):.1f}%)\")\n",
    "    \n",
    "    # Check if we have enough samples\n",
    "    if n_aug < 10 or n_non_aug < 10:\n",
    "        print(f\"\\n⚠️  ERROR: Insufficient samples in one class!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING CLASSIFICATION MODELS ON COMBINED DATASET\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results_df = combined_df.copy()\n",
    "    \n",
    "    # Convert AUG to binary (0=False, 1=True)\n",
    "    y = combined_df['AUG'].astype(int).values\n",
    "    \n",
    "    # Descriptor types - ONLY AVALON\n",
    "    descriptor_types = ['avalon']\n",
    "    \n",
    "    # Stratified K-Fold for balanced splits\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(skf.split(combined_df, y)):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FOLD {fold_idx + 1}/{n_folds}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        train_df = combined_df.iloc[train_idx]\n",
    "        test_df = combined_df.iloc[test_idx]\n",
    "        y_train = y[train_idx]\n",
    "        y_test = y[test_idx]\n",
    "        \n",
    "        print(f\"Train: {len(train_df)} compounds (Aug={y_train.sum()}, {100*y_train.mean():.1f}%)\")\n",
    "        print(f\"Test:  {len(test_df)} compounds (Aug={y_test.sum()}, {100*y_test.mean():.1f}%)\")\n",
    "        \n",
    "        # For each descriptor type\n",
    "        for descriptor_type in descriptor_types:\n",
    "            print(f\"\\n--- Descriptor: {descriptor_type.upper()} ---\")\n",
    "            \n",
    "            # Generate features\n",
    "            print(\"  Generating features...\")\n",
    "            if descriptor_type == 'avalon':\n",
    "                X_train, valid_train = generate_avalon_features(train_df['SMILES'].tolist())\n",
    "                X_test, valid_test = generate_avalon_features(test_df['SMILES'].tolist())\n",
    "                \n",
    "            elif descriptor_type == 'rdkit2d':\n",
    "                X_train, valid_train = generate_rdkit2d_features(train_df['SMILES'].tolist())\n",
    "                X_test, valid_test = generate_rdkit2d_features(test_df['SMILES'].tolist())\n",
    "                \n",
    "            elif descriptor_type == 'ecfp6':\n",
    "                X_train, valid_train = generate_ecfp6_features(train_df['SMILES'].tolist())\n",
    "                X_test, valid_test = generate_ecfp6_features(test_df['SMILES'].tolist())\n",
    "            \n",
    "            print(f\"  Train features: {X_train.shape}\")\n",
    "            print(f\"  Test features:  {X_test.shape}\")\n",
    "            \n",
    "            # Filter valid samples\n",
    "            y_train_valid = y_train[valid_train]\n",
    "            y_test_valid = y_test[valid_test]\n",
    "            test_indices = test_df.index[valid_test]\n",
    "            \n",
    "            # For each replicate\n",
    "            for replicate in range(n_replicates):\n",
    "                print(f\"  Training replicate {replicate + 1}/{n_replicates}...\", end=' ')\n",
    "                \n",
    "                # Set seed for this replicate\n",
    "                rep_seed = random_seed + fold_idx * 100 + replicate\n",
    "                \n",
    "                # Train XGBoost classifier\n",
    "                model = xgb.XGBClassifier(\n",
    "                    n_estimators=100,\n",
    "                    max_depth=6,\n",
    "                    learning_rate=0.1,\n",
    "                    random_state=rep_seed,\n",
    "                    n_jobs=-1,\n",
    "                    eval_metric='logloss'\n",
    "                )\n",
    "                \n",
    "                model.fit(X_train, y_train_valid)\n",
    "                \n",
    "                # Get probability predictions (probability of being augmented, class 1)\n",
    "                pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "                \n",
    "                # Store predictions (probabilities)\n",
    "                col_name = f'pred_fold{fold_idx+1}_{descriptor_type}_rep{replicate+1}_proba'\n",
    "                \n",
    "                # Initialize column if needed\n",
    "                if col_name not in results_df.columns:\n",
    "                    results_df[col_name] = np.nan\n",
    "                \n",
    "                # Store predictions at correct indices\n",
    "                results_df.loc[test_indices, col_name] = pred_proba\n",
    "                \n",
    "                # Calculate metrics for this fold/replicate\n",
    "                pred_binary = (pred_proba >= 0.5).astype(int)\n",
    "                bal_acc = balanced_accuracy_score(y_test_valid, pred_binary)\n",
    "                mcc = matthews_corrcoef(y_test_valid, pred_binary)\n",
    "                \n",
    "                print(f\"BalAcc={bal_acc:.3f}, MCC={mcc:.3f}\")\n",
    "    \n",
    "    # Calculate aggregate metrics\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"AGGREGATE PERFORMANCE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    pred_cols = [c for c in results_df.columns if c.startswith('pred_')]\n",
    "    \n",
    "    if pred_cols:\n",
    "        # Average predictions across all models\n",
    "        results_df['mean_proba'] = results_df[pred_cols].mean(axis=1)\n",
    "        results_df['std_proba'] = results_df[pred_cols].std(axis=1)\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        y_true = results_df['AUG'].astype(int).values\n",
    "        y_pred_proba = results_df['mean_proba'].values\n",
    "        y_pred_binary = (y_pred_proba >= 0.5).astype(int)\n",
    "        \n",
    "        bal_acc = balanced_accuracy_score(y_true, y_pred_binary)\n",
    "        mcc = matthews_corrcoef(y_true, y_pred_binary)\n",
    "        precision = precision_score(y_true, y_pred_binary, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred_binary, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n",
    "        \n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_pred_proba)\n",
    "            ap = average_precision_score(y_true, y_pred_proba)\n",
    "        except:\n",
    "            auc = np.nan\n",
    "            ap = np.nan\n",
    "        \n",
    "        print(f\"\\nOverall Performance (Combined Model):\")\n",
    "        print(f\"  Balanced Accuracy: {bal_acc:.4f}\")\n",
    "        print(f\"  MCC:               {mcc:.4f}\")\n",
    "        print(f\"  Precision:         {precision:.4f}\")\n",
    "        print(f\"  Recall:            {recall:.4f}\")\n",
    "        print(f\"  F1-Score:          {f1:.4f}\")\n",
    "        print(f\"  ROC-AUC:           {auc:.4f}\")\n",
    "        print(f\"  Avg Prec:          {ap:.4f}\")\n",
    "        \n",
    "        # Performance by dataset\n",
    "        print(f\"\\nPerformance by Dataset Source:\")\n",
    "        print(f\"{'Dataset':<40} {'N':<8} {'BalAcc':<8} {'MCC':<8} {'F1':<8} {'AUC':<8}\")\n",
    "        print(\"-\" * 90)\n",
    "        \n",
    "        for dataset_name in results_df['dataset_source'].unique():\n",
    "            mask = results_df['dataset_source'] == dataset_name\n",
    "            df_subset = results_df[mask]\n",
    "            \n",
    "            y_true_subset = df_subset['AUG'].astype(int).values\n",
    "            y_pred_proba_subset = df_subset['mean_proba'].values\n",
    "            y_pred_binary_subset = (y_pred_proba_subset >= 0.5).astype(int)\n",
    "            \n",
    "            bal_acc_subset = balanced_accuracy_score(y_true_subset, y_pred_binary_subset)\n",
    "            mcc_subset = matthews_corrcoef(y_true_subset, y_pred_binary_subset)\n",
    "            f1_subset = f1_score(y_true_subset, y_pred_binary_subset, zero_division=0)\n",
    "            \n",
    "            try:\n",
    "                auc_subset = roc_auc_score(y_true_subset, y_pred_proba_subset)\n",
    "            except:\n",
    "                auc_subset = np.nan\n",
    "            \n",
    "            print(f\"{dataset_name:<40} {len(df_subset):<8} \"\n",
    "                  f\"{bal_acc_subset:<8.3f} {mcc_subset:<8.3f} {f1_subset:<8.3f} {auc_subset:<8.3f}\")\n",
    "    \n",
    "    # Save results\n",
    "    output_file = Path(output_dir) / 'combined_recognizability_predictions.parquet'\n",
    "    results_df.to_parquet(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\n✓ Results saved to: {output_file}\")\n",
    "    print(f\"  Total compounds: {len(results_df)}\")\n",
    "    print(f\"  Total columns: {len(results_df.columns)}\")\n",
    "    print(f\"  Prediction columns: {len(pred_cols)}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# ============================================================================\n",
    "# Generate Summary Report\n",
    "# ============================================================================\n",
    "\n",
    "def generate_recognizability_report_combined(data_dir='../data/recognizability'):\n",
    "    \"\"\"\n",
    "    Generate a summary report of combined recognizability performance\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dir : str\n",
    "        Directory containing prediction results\n",
    "    \"\"\"\n",
    "    \n",
    "    result_file = Path(data_dir) / 'combined_recognizability_predictions.parquet'\n",
    "    \n",
    "    if not result_file.exists():\n",
    "        print(\"No combined prediction file found\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOGNIZABILITY PERFORMANCE SUMMARY (COMBINED MODEL)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    df = pd.read_parquet(result_file)\n",
    "    \n",
    "    # Check if we have predictions\n",
    "    if 'mean_proba' not in df.columns:\n",
    "        print(\"No predictions found in results file\")\n",
    "        return\n",
    "    \n",
    "    # Overall performance\n",
    "    y_true = df['AUG'].astype(int).values\n",
    "    y_pred_proba = df['mean_proba'].values\n",
    "    y_pred_binary = (y_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred_binary)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred_binary)\n",
    "    precision = precision_score(y_true, y_pred_binary, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred_binary, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_pred_proba)\n",
    "        ap = average_precision_score(y_true, y_pred_proba)\n",
    "    except:\n",
    "        auc = np.nan\n",
    "        ap = np.nan\n",
    "    \n",
    "    print(f\"\\nOVERALL PERFORMANCE:\")\n",
    "    print(f\"  Total Compounds:    {len(df)}\")\n",
    "    print(f\"  Augmented:          {y_true.sum()} ({100*y_true.mean():.1f}%)\")\n",
    "    print(f\"  Non-augmented:      {len(df) - y_true.sum()} ({100*(1-y_true.mean()):.1f}%)\")\n",
    "    print(f\"\\n  Balanced Accuracy:  {bal_acc:.4f}\")\n",
    "    print(f\"  MCC:                {mcc:.4f}\")\n",
    "    print(f\"  Precision:          {precision:.4f}\")\n",
    "    print(f\"  Recall:             {recall:.4f}\")\n",
    "    print(f\"  F1-Score:           {f1:.4f}\")\n",
    "    print(f\"  ROC-AUC:            {auc:.4f}\")\n",
    "    print(f\"  Average Precision:  {ap:.4f}\")\n",
    "    \n",
    "    # Performance by dataset\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PERFORMANCE BY DATASET SOURCE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for dataset_name in sorted(df['dataset_source'].unique()):\n",
    "        mask = df['dataset_source'] == dataset_name\n",
    "        df_subset = df[mask]\n",
    "        \n",
    "        y_true_subset = df_subset['AUG'].astype(int).values\n",
    "        y_pred_proba_subset = df_subset['mean_proba'].values\n",
    "        y_pred_binary_subset = (y_pred_proba_subset >= 0.5).astype(int)\n",
    "        \n",
    "        bal_acc_subset = balanced_accuracy_score(y_true_subset, y_pred_binary_subset)\n",
    "        mcc_subset = matthews_corrcoef(y_true_subset, y_pred_binary_subset)\n",
    "        precision_subset = precision_score(y_true_subset, y_pred_binary_subset, zero_division=0)\n",
    "        recall_subset = recall_score(y_true_subset, y_pred_binary_subset, zero_division=0)\n",
    "        f1_subset = f1_score(y_true_subset, y_pred_binary_subset, zero_division=0)\n",
    "        \n",
    "        try:\n",
    "            auc_subset = roc_auc_score(y_true_subset, y_pred_proba_subset)\n",
    "            ap_subset = average_precision_score(y_true_subset, y_pred_proba_subset)\n",
    "        except:\n",
    "            auc_subset = np.nan\n",
    "            ap_subset = np.nan\n",
    "        \n",
    "        summary_data.append({\n",
    "            'dataset': dataset_name,\n",
    "            'n_samples': len(df_subset),\n",
    "            'n_augmented': y_true_subset.sum(),\n",
    "            'pct_augmented': 100 * y_true_subset.mean(),\n",
    "            'balanced_accuracy': bal_acc_subset,\n",
    "            'mcc': mcc_subset,\n",
    "            'precision': precision_subset,\n",
    "            'recall': recall_subset,\n",
    "            'f1_score': f1_subset,\n",
    "            'roc_auc': auc_subset,\n",
    "            'avg_precision': ap_subset\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df = summary_df.sort_values('roc_auc', ascending=False)\n",
    "    \n",
    "    print(f\"\\n{'Dataset':<40} {'N':<8} {'%Aug':<8} {'BalAcc':<8} {'MCC':<8} {'F1':<8} {'AUC':<8}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for _, row in summary_df.iterrows():\n",
    "        print(f\"{row['dataset']:<40} {row['n_samples']:<8.0f} \"\n",
    "              f\"{row['pct_augmented']:<8.1f} {row['balanced_accuracy']:<8.3f} \"\n",
    "              f\"{row['mcc']:<8.3f} {row['f1_score']:<8.3f} {row['roc_auc']:<8.3f}\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary_path = Path(data_dir) / 'recognizability_combined_summary.csv'\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    print(f\"\\n✓ Summary saved to: {summary_path}\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def print_all_performance(data_dir='../data/recognizability'):\n",
    "    \"\"\"\n",
    "    Print comprehensive performance report from saved predictions\n",
    "    Use this to view all performance metrics after training is complete\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dir : str\n",
    "        Directory containing prediction results\n",
    "    \n",
    "    Usage:\n",
    "    ------\n",
    "    # After training, just call this to see all results:\n",
    "    from recognizability_analysis import print_all_performance\n",
    "    print_all_performance('../data/recognizability')\n",
    "    \"\"\"\n",
    "    \n",
    "    result_file = Path(data_dir) / 'combined_recognizability_predictions.parquet'\n",
    "    \n",
    "    if not result_file.exists():\n",
    "        print(f\"❌ No results file found at: {result_file}\")\n",
    "        print(\"   Run the training pipeline first!\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPLETE RECOGNIZABILITY PERFORMANCE REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load results\n",
    "    df = pd.read_parquet(result_file)\n",
    "    \n",
    "    if 'mean_proba' not in df.columns:\n",
    "        print(\"❌ No predictions found in results file\")\n",
    "        return None\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 1. OVERALL STATISTICS\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"1. DATASET OVERVIEW\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nTotal unique compounds: {len(df):,}\")\n",
    "    print(f\"Number of datasets:     {df['dataset_source'].nunique()}\")\n",
    "    \n",
    "    n_aug = (df['AUG'] == True).sum()\n",
    "    n_non_aug = (df['AUG'] == False).sum()\n",
    "    \n",
    "    print(f\"\\nClass Distribution:\")\n",
    "    print(f\"  Non-augmented: {n_non_aug:,} ({100*n_non_aug/len(df):.2f}%)\")\n",
    "    print(f\"  Augmented:     {n_aug:,} ({100*n_aug/len(df):.2f}%)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 2. OVERALL MODEL PERFORMANCE\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"2. OVERALL MODEL PERFORMANCE (All Datasets Combined)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    y_true = df['AUG'].astype(int).values\n",
    "    y_pred_proba = df['mean_proba'].values\n",
    "    y_pred_binary = (y_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred_binary)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred_binary)\n",
    "    precision = precision_score(y_true, y_pred_binary, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred_binary, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_pred_proba)\n",
    "        ap = average_precision_score(y_true, y_pred_proba)\n",
    "    except:\n",
    "        auc = np.nan\n",
    "        ap = np.nan\n",
    "    \n",
    "    print(f\"\\nClassification Metrics:\")\n",
    "    print(f\"  Balanced Accuracy:  {bal_acc:.4f}\")\n",
    "    print(f\"  MCC:                {mcc:.4f}\")\n",
    "    print(f\"  Precision:          {precision:.4f}\")\n",
    "    print(f\"  Recall:             {recall:.4f}\")\n",
    "    print(f\"  F1-Score:           {f1:.4f}\")\n",
    "    print(f\"  ROC-AUC:            {auc:.4f}\")\n",
    "    print(f\"  Average Precision:  {ap:.4f}\")\n",
    "    \n",
    "    # Prediction uncertainty\n",
    "    print(f\"\\nPrediction Statistics:\")\n",
    "    print(f\"  Mean probability:   {y_pred_proba.mean():.4f}\")\n",
    "    print(f\"  Std probability:    {y_pred_proba.std():.4f}\")\n",
    "    print(f\"  Mean uncertainty:   {df['std_proba'].mean():.4f}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 3. PERFORMANCE BY DESCRIPTOR TYPE\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"3. DESCRIPTOR TYPE: AVALON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get all avalon columns\n",
    "    avalon_cols = [c for c in df.columns if '_avalon_' in c and 'pred_' in c]\n",
    "    \n",
    "    if avalon_cols:\n",
    "        # Average predictions for avalon\n",
    "        avalon_proba = df[avalon_cols].mean(axis=1).values\n",
    "        avalon_binary = (avalon_proba >= 0.5).astype(int)\n",
    "        \n",
    "        bal_acc_avalon = balanced_accuracy_score(y_true, avalon_binary)\n",
    "        mcc_avalon = matthews_corrcoef(y_true, avalon_binary)\n",
    "        f1_avalon = f1_score(y_true, avalon_binary, zero_division=0)\n",
    "        \n",
    "        try:\n",
    "            auc_avalon = roc_auc_score(y_true, avalon_proba)\n",
    "        except:\n",
    "            auc_avalon = np.nan\n",
    "        \n",
    "        print(f\"\\nAVALON Fingerprints:\")\n",
    "        print(f\"  Models:            {len(avalon_cols)}\")\n",
    "        print(f\"  Balanced Accuracy: {bal_acc_avalon:.4f}\")\n",
    "        print(f\"  MCC:               {mcc_avalon:.4f}\")\n",
    "        print(f\"  F1-Score:          {f1_avalon:.4f}\")\n",
    "        print(f\"  ROC-AUC:           {auc_avalon:.4f}\")\n",
    "    else:\n",
    "        print(\"\\nNo Avalon predictions found\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 4. PERFORMANCE BY DATASET\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"4. PERFORMANCE BY DATASET SOURCE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    dataset_results = []\n",
    "    \n",
    "    for dataset_name in sorted(df['dataset_source'].unique()):\n",
    "        mask = df['dataset_source'] == dataset_name\n",
    "        df_subset = df[mask]\n",
    "        \n",
    "        y_true_subset = df_subset['AUG'].astype(int).values\n",
    "        y_pred_proba_subset = df_subset['mean_proba'].values\n",
    "        y_pred_binary_subset = (y_pred_proba_subset >= 0.5).astype(int)\n",
    "        \n",
    "        bal_acc_subset = balanced_accuracy_score(y_true_subset, y_pred_binary_subset)\n",
    "        mcc_subset = matthews_corrcoef(y_true_subset, y_pred_binary_subset)\n",
    "        precision_subset = precision_score(y_true_subset, y_pred_binary_subset, zero_division=0)\n",
    "        recall_subset = recall_score(y_true_subset, y_pred_binary_subset, zero_division=0)\n",
    "        f1_subset = f1_score(y_true_subset, y_pred_binary_subset, zero_division=0)\n",
    "        \n",
    "        try:\n",
    "            auc_subset = roc_auc_score(y_true_subset, y_pred_proba_subset)\n",
    "            ap_subset = average_precision_score(y_true_subset, y_pred_proba_subset)\n",
    "        except:\n",
    "            auc_subset = np.nan\n",
    "            ap_subset = np.nan\n",
    "        \n",
    "        dataset_results.append({\n",
    "            'dataset': dataset_name,\n",
    "            'n_samples': len(df_subset),\n",
    "            'n_augmented': y_true_subset.sum(),\n",
    "            'pct_augmented': 100 * y_true_subset.mean(),\n",
    "            'balanced_accuracy': bal_acc_subset,\n",
    "            'mcc': mcc_subset,\n",
    "            'precision': precision_subset,\n",
    "            'recall': recall_subset,\n",
    "            'f1_score': f1_subset,\n",
    "            'roc_auc': auc_subset,\n",
    "            'avg_precision': ap_subset\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(dataset_results)\n",
    "    \n",
    "    # Sort by ROC-AUC\n",
    "    results_df = results_df.sort_values('roc_auc', ascending=False)\n",
    "    \n",
    "    print(f\"\\n{'Dataset':<40} {'N':>7} {'%Aug':>6} {'BalAcc':>7} {'MCC':>7} {'Prec':>6} {'Rec':>6} {'F1':>6} {'AUC':>6}\")\n",
    "    print(\"-\" * 105)\n",
    "    \n",
    "    for _, row in results_df.iterrows():\n",
    "        print(f\"{row['dataset']:<40} {row['n_samples']:>7.0f} \"\n",
    "              f\"{row['pct_augmented']:>6.1f} {row['balanced_accuracy']:>7.3f} \"\n",
    "              f\"{row['mcc']:>7.3f} {row['precision']:>6.3f} {row['recall']:>6.3f} \"\n",
    "              f\"{row['f1_score']:>6.3f} {row['roc_auc']:>6.3f}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 5. CROSS-VALIDATION STATISTICS\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"5. CROSS-VALIDATION CONSISTENCY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    pred_cols = [c for c in df.columns if c.startswith('pred_fold')]\n",
    "    n_folds = len(set([c.split('_')[0] for c in pred_cols if 'fold' in c]))\n",
    "    n_replicates = len([c for c in pred_cols if 'fold1' in c and 'avalon' in c])\n",
    "    \n",
    "    print(f\"\\nCross-validation setup:\")\n",
    "    print(f\"  Number of folds:      {n_folds}\")\n",
    "    print(f\"  Descriptor type:      Avalon\")\n",
    "    print(f\"  Replicates per fold:  {n_replicates}\")\n",
    "    print(f\"  Total models:         {len(pred_cols)}\")\n",
    "    \n",
    "    print(f\"\\nPrediction consistency:\")\n",
    "    print(f\"  Mean std dev:         {df['std_proba'].mean():.4f}\")\n",
    "    print(f\"  Max std dev:          {df['std_proba'].max():.4f}\")\n",
    "    print(f\"  Min std dev:          {df['std_proba'].min():.4f}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 6. SUMMARY STATISTICS\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"6. SUMMARY STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nPer-Dataset Performance (mean ± std):\")\n",
    "    print(f\"  Balanced Accuracy: {results_df['balanced_accuracy'].mean():.4f} ± {results_df['balanced_accuracy'].std():.4f}\")\n",
    "    print(f\"  MCC:               {results_df['mcc'].mean():.4f} ± {results_df['mcc'].std():.4f}\")\n",
    "    print(f\"  Precision:         {results_df['precision'].mean():.4f} ± {results_df['precision'].std():.4f}\")\n",
    "    print(f\"  Recall:            {results_df['recall'].mean():.4f} ± {results_df['recall'].std():.4f}\")\n",
    "    print(f\"  F1-Score:          {results_df['f1_score'].mean():.4f} ± {results_df['f1_score'].std():.4f}\")\n",
    "    print(f\"  ROC-AUC:           {results_df['roc_auc'].mean():.4f} ± {results_df['roc_auc'].std():.4f}\")\n",
    "    \n",
    "    print(f\"\\nBest performing dataset:  {results_df.iloc[0]['dataset']} (AUC={results_df.iloc[0]['roc_auc']:.4f}, MCC={results_df.iloc[0]['mcc']:.4f})\")\n",
    "    print(f\"Worst performing dataset: {results_df.iloc[-1]['dataset']} (AUC={results_df.iloc[-1]['roc_auc']:.4f}, MCC={results_df.iloc[-1]['mcc']:.4f})\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 7. FILES GENERATED\n",
    "    # ========================================================================\n",
    "\n",
    "    \n",
    "    summary_file = Path(data_dir) / 'recognizability_combined_summary.csv'\n",
    "    \n",
    "    print(f\"\\nResults saved to:\")\n",
    "    print(f\"  Predictions: {result_file}\")\n",
    "    if summary_file.exists():\n",
    "        print(f\"  Summary CSV: {summary_file}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Main Execution\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Step 1: Prepare data\n",
    "    print(\"\\nStep 1: Preparing data...\")\n",
    "    cleaned_datasets = prepare_recognizability_data(\n",
    "        augmented_data_dir='../data/augment_all',\n",
    "        output_dir='../data/recognizability'\n",
    "    )\n",
    "    \n",
    "    # Step 2: Run classification on COMBINED dataset\n",
    "    print(\"\\nStep 2: Running classification models on combined dataset...\")\n",
    "    run_recognizability_classification_combined(\n",
    "        data_dir='../data/recognizability',\n",
    "        output_dir='../data/recognizability',\n",
    "        n_folds=5,\n",
    "        n_replicates=3,\n",
    "        random_seed=42\n",
    "    )\n",
    "    \n",
    "    # Step 3: Generate report\n",
    "    print(\"\\nStep 3: Generating summary report...\")\n",
    "    generate_recognizability_report_combined(\n",
    "        data_dir='../data/recognizability'\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914b52a3-9c14-47a6-b0e6-4430456dd13f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_res = pd.read_csv('../data/recognizability/recognizability_combined_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0477626a-312a-4017-894d-bf7e681a810a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3., 0., 2., 2., 3., 0., 3., 0., 0., 2.]),\n",
       " array([0.49761779, 0.54785601, 0.59809423, 0.64833245, 0.69857067,\n",
       "        0.74880889, 0.79904711, 0.84928534, 0.89952356, 0.94976178,\n",
       "        1.        ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(df_res['balanced_accuracy'].tolist(), bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a0f9e-11e4-46ef-9cda-6974fc006984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a58f1f7-f499-412a-bba7-09822b6de9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372e5ef-d824-4bdc-abf8-54ebc026e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091056eb-8483-4439-b7c9-3275e4c62f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af14b346-459a-4a9e-a748-3828f6a7c041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bb5a0f-3267-41d6-a80a-01af26197fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74636650-16d8-48c0-b396-dc0ec40c3495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308cf0d6-5456-4143-9310-3b01bf9d0601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88592cd0-72ca-43ff-bf26-3dfb40441020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e84c1b-b5ae-40c2-b19f-32df36b62835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b99e46-aa33-4ac6-a3ac-7f09334903c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e84e64-fbb0-4428-9136-1717f1a22b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851bab0d-2767-4b89-8b0f-bb1597cdf515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-chemprop_MTL-chemprop_mtl",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "chemprop_MTL",
   "language": "python",
   "name": "conda-env-chemprop_MTL-chemprop_mtl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
