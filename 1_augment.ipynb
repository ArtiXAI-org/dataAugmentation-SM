{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "471532e1-30b2-4352-bce7-ddaa19266572",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MMP-Based Data Augmentation for Drug Discovery\n",
    "A comprehensive class for expanding molecular datasets using Matched Molecular Pairs\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import AllChem, inchi\n",
    "import subprocess\n",
    "import tempfile\n",
    "import base64\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Optional, Union\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from scipy.stats import pearsonr\n",
    "import time\n",
    "\n",
    "\n",
    "# Suppress RDKit warnings\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"rdkit\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"rdkit\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"rdkit\")\n",
    "\n",
    "\n",
    "class MMPDataAugmentor:\n",
    "    \"\"\"\n",
    "    Complete pipeline for MMP-based data augmentation.\n",
    "    \n",
    "    This class handles:\n",
    "    1. Molecule fragmentation\n",
    "    2. MMP identification\n",
    "    3. Data augmentation through matched pairs\n",
    "    4. Chemical structure generation\n",
    "    5. Quality control and filtering\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe containing molecules and properties\n",
    "    smiles_col : str\n",
    "        Name of the column containing SMILES strings\n",
    "    target_cols : Union[str, List[str]]\n",
    "        Name(s) of target column(s) to augment\n",
    "    mmpa_dir : str, default='./mmpa'\n",
    "        Path to MMPA scripts directory\n",
    "    symmetric : bool, default=True\n",
    "        Generate symmetric MMPs (A->B and B->A)\n",
    "    max_heavy : int, default=15\n",
    "        Maximum heavy atom change allowed\n",
    "    max_ratio : float, default=0.35\n",
    "        Maximum ratio of change relative to molecule size\n",
    "    min_common : int, default=4\n",
    "        Minimum common MMPs required between scaffold pairs\n",
    "    pearson_thresh : float, default=0.3\n",
    "        Minimum Pearson correlation for scaffold pairing\n",
    "    crmsd_thresh : float, default=0.8\n",
    "        Maximum cRMSD for scaffold pairing\n",
    "    std_threshold : float, default=0.8\n",
    "        Maximum standard deviation for keeping augmented data\n",
    "    verbose : bool, default=True\n",
    "        Print progress messages\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        smiles_col: str = \"SMILES\",\n",
    "        target_cols: Union[str, List[str]] = \"Y\",\n",
    "        mmpa_dir: str = '../mmpa',\n",
    "        symmetric: bool = True,\n",
    "        max_heavy: int = 15,\n",
    "        max_ratio: float = 0.35,\n",
    "        min_common: int = 4,\n",
    "        pearson_thresh: float = 0.3,\n",
    "        crmsd_thresh: float = 0.8,\n",
    "        std_threshold: float = 0.8,\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "        self.df_original = df.copy()\n",
    "        self.smiles_col = smiles_col\n",
    "        self.target_cols = [target_cols] if isinstance(target_cols, str) else target_cols\n",
    "        self.mmpa_dir = mmpa_dir\n",
    "        self.symmetric = symmetric\n",
    "        self.max_heavy = max_heavy\n",
    "        self.max_ratio = max_ratio\n",
    "        self.min_common = min_common\n",
    "        self.pearson_thresh = pearson_thresh\n",
    "        self.crmsd_thresh = crmsd_thresh\n",
    "        self.std_threshold = std_threshold\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Initialize storage for results\n",
    "        self.augmented_df = None\n",
    "        self.statistics = {}\n",
    "        \n",
    "        # Validate inputs\n",
    "        self._validate_inputs()\n",
    "    \n",
    "    def _validate_inputs(self):\n",
    "        \"\"\"Validate input parameters and data\"\"\"\n",
    "        if self.smiles_col not in self.df_original.columns:\n",
    "            raise ValueError(f\"SMILES column '{self.smiles_col}' not found in dataframe\")\n",
    "        \n",
    "        for col in self.target_cols:\n",
    "            if col not in self.df_original.columns:\n",
    "                raise ValueError(f\"Target column '{col}' not found in dataframe\")\n",
    "        \n",
    "        if not os.path.exists(self.mmpa_dir):\n",
    "            raise ValueError(f\"MMPA directory '{self.mmpa_dir}' not found\")\n",
    "    \n",
    "    def _log(self, message: str):\n",
    "        \"\"\"Print message if verbose mode is enabled\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"[MMPAugmentor] {message}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _encode_string(s: str) -> str:\n",
    "        \"\"\"Encode string to base64\"\"\"\n",
    "        return base64.urlsafe_b64encode(s.encode()).decode()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _smiles_to_inchikey(smiles: str) -> Optional[str]:\n",
    "        \"\"\"Convert SMILES to InChIKey\"\"\"\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        try:\n",
    "            return inchi.MolToInchiKey(mol)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def _fragment_molecules(self, output_csv: str):\n",
    "        \"\"\"\n",
    "        Fragment molecules using MMPA rfrag.py\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        output_csv : str\n",
    "            Path to output CSV file\n",
    "        \"\"\"\n",
    "        self._log(\"0) Generating molecular fragments...\")\n",
    "        \n",
    "        # Prepare data for fragmentation\n",
    "        df_prep = self.df_original[[self.smiles_col] + self.target_cols].copy()\n",
    "        df_prep['ID'] = [self._smiles_to_inchikey(smi) for smi in df_prep[self.smiles_col]]\n",
    "        \n",
    "        # Create temporary directory\n",
    "        with tempfile.TemporaryDirectory() as tmp:\n",
    "            smi_path = os.path.join(tmp, 'input.smi')\n",
    "            frag_path = os.path.join(tmp, 'fragmented.txt')\n",
    "            mmps_path = os.path.join(tmp, 'mmps.csv')\n",
    "            smirks_path = os.path.join(tmp, 'smirks.txt')\n",
    "            cansmirks_path = os.path.join(tmp, 'cansmirks.txt')\n",
    "            \n",
    "            # Write SMILES file\n",
    "            df_prep[[self.smiles_col, 'ID']].to_csv(\n",
    "                smi_path, index=False, sep=' ', header=False\n",
    "            )\n",
    "            \n",
    "            # Fragment molecules\n",
    "            with open(frag_path, 'w') as out:\n",
    "                subprocess.run(\n",
    "                    ['python', f'{self.mmpa_dir}/rfrag.py'],\n",
    "                    stdin=open(smi_path),\n",
    "                    stdout=out\n",
    "                )\n",
    "            \n",
    "            self._log(\"1) Indexing fragments and generating MMPs...\")\n",
    "            \n",
    "            # Build MMPs\n",
    "            cmd = ['python', f'{self.mmpa_dir}/indexing.py']\n",
    "            if self.symmetric:\n",
    "                cmd.append('-s')\n",
    "            if self.max_heavy:\n",
    "                cmd.extend(['-m', str(self.max_heavy)])\n",
    "            if self.max_ratio:\n",
    "                cmd.extend(['-r', str(self.max_ratio)])\n",
    "            \n",
    "            with open(mmps_path, 'w') as out:\n",
    "                subprocess.run(cmd, stdin=open(frag_path), stdout=out)\n",
    "            \n",
    "            # Read and process MMPs\n",
    "            with open(mmps_path) as f:\n",
    "                lines = [line.strip() for line in f if line.strip()]\n",
    "            \n",
    "            splits = [line.split(',') for line in lines]\n",
    "            df_mmps = pd.DataFrame(\n",
    "                splits,\n",
    "                columns=['L_SMILES', 'R_SMILES', 'L_ID', 'R_ID', 'SMIRKS', 'CORE']\n",
    "            )\n",
    "            \n",
    "            # Map target values\n",
    "            for target_col in self.target_cols:\n",
    "                y_map = df_prep.set_index('ID')[target_col].to_dict()\n",
    "                df_mmps[f'L_{target_col}'] = df_mmps['L_ID'].map(y_map)\n",
    "                df_mmps[f'R_{target_col}'] = df_mmps['R_ID'].map(y_map)\n",
    "                df_mmps[f'Delta_{target_col}'] = df_mmps[f'R_{target_col}'] - df_mmps[f'L_{target_col}']\n",
    "            \n",
    "            # Filter valid SMIRKS\n",
    "            df_mmps = df_mmps[df_mmps['SMIRKS'].apply(\n",
    "                lambda x: isinstance(x, str) and '>>' in x\n",
    "            )]\n",
    "            \n",
    "            self._log(\"2) Canonicalizing SMIRKS...\")\n",
    "            \n",
    "            # Canonicalize SMIRKS\n",
    "            df_mmps['__row'] = range(len(df_mmps))\n",
    "            df_mmps[['SMIRKS', '__row']].to_csv(\n",
    "                smirks_path, index=False, sep=' ', header=False\n",
    "            )\n",
    "            \n",
    "            with open(cansmirks_path, 'w') as out:\n",
    "                subprocess.run(\n",
    "                    ['python', f'{self.mmpa_dir}/cansmirk.py'],\n",
    "                    stdin=open(smirks_path),\n",
    "                    stdout=out\n",
    "                )\n",
    "            \n",
    "            canon_df = pd.read_csv(\n",
    "                cansmirks_path, sep=' ', names=['Canonical_SMIRKS', 'index']\n",
    "            )\n",
    "            \n",
    "            df_mmps = df_mmps.merge(\n",
    "                canon_df, left_on='__row', right_on='index'\n",
    "            ).drop(columns=['__row', 'index'])\n",
    "            \n",
    "            # Split canonical SMIRKS\n",
    "            df_mmps[['L_sub', 'R_sub']] = df_mmps['Canonical_SMIRKS'].str.split(\n",
    "                '>>', expand=True\n",
    "            )\n",
    "            \n",
    "            # Add encoded IDs\n",
    "            df_mmps['L_sub_ID'] = [self._encode_string(k) for k in df_mmps['L_sub']]\n",
    "            df_mmps['R_sub_ID'] = [self._encode_string(k) for k in df_mmps['R_sub']]\n",
    "            df_mmps['SMIRKS_ID'] = [self._encode_string(k) for k in df_mmps['Canonical_SMIRKS']]\n",
    "            df_mmps['CORE_ID'] = [self._encode_string(k) for k in df_mmps['CORE']]\n",
    "            \n",
    "            df_mmps = df_mmps.drop_duplicates()\n",
    "            \n",
    "            # Save to output\n",
    "            df_mmps.to_csv(output_csv, index=False)\n",
    "        \n",
    "        return df_mmps\n",
    "    \n",
    "    def _augment_data(self, df_mmps: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Augment data using matched molecular pairs\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df_mmps : pd.DataFrame\n",
    "            DataFrame containing MMPs\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Augmented dataset\n",
    "        \"\"\"\n",
    "        self._log(\"3) Computing pairwise scaffold correlations...\")\n",
    "        \n",
    "        # Group by CORE\n",
    "        series = {core: group for core, group in df_mmps.groupby(\"CORE\")}\n",
    "        \n",
    "        # Compute pairwise scores\n",
    "        pair_scores = []\n",
    "        series_items = list(series.items())\n",
    "        total_combinations = len(series_items) * (len(series_items) - 1) // 2\n",
    "        \n",
    "        for (core1, df1), (core2, df2) in tqdm(\n",
    "            combinations(series_items, 2),\n",
    "            total=total_combinations,\n",
    "            desc=\"Computing correlations\",\n",
    "            disable=not self.verbose\n",
    "        ):\n",
    "            subs1 = set(df1[\"L_sub\"])\n",
    "            subs2 = set(df2[\"L_sub\"])\n",
    "            common = subs1 & subs2\n",
    "            \n",
    "            if len(common) < self.min_common:\n",
    "                continue\n",
    "            \n",
    "            merged = pd.merge(\n",
    "                df1, df2,\n",
    "                left_on=[\"L_sub\", \"R_sub\"],\n",
    "                right_on=[\"L_sub\", \"R_sub\"],\n",
    "                suffixes=('_1', '_2')\n",
    "            )\n",
    "            \n",
    "            if len(merged) < self.min_common:\n",
    "                continue\n",
    "            \n",
    "            # Use first target column for filtering\n",
    "            target_col = self.target_cols[0]\n",
    "            y1 = merged[f'Delta_{target_col}_1'].values\n",
    "            y2 = merged[f'Delta_{target_col}_2'].values\n",
    "            crmsd = np.sqrt(np.mean((y1 - y2) ** 2))\n",
    "            \n",
    "            try:\n",
    "                corr = pearsonr(y1, y2)[0]\n",
    "            except:\n",
    "                corr = np.nan\n",
    "            \n",
    "            pair_scores.append((core1, core2, crmsd, corr, len(merged)))\n",
    "        \n",
    "        # Filter pairs\n",
    "        filtered_pairs = [\n",
    "            (s1, s2) for s1, s2, rmsd, corr, n in pair_scores\n",
    "            if rmsd <= self.crmsd_thresh and (not np.isnan(corr) and corr >= self.pearson_thresh)\n",
    "        ]\n",
    "        \n",
    "        self._log(f\"4) Found {len(filtered_pairs)} valid scaffold pairs. Generating augmented data...\")\n",
    "        \n",
    "        # Augment data\n",
    "        augmented_entries = []\n",
    "        \n",
    "        for s1, s2 in tqdm(filtered_pairs, desc=\"Augmenting\", disable=not self.verbose):\n",
    "            df1 = series[s1]\n",
    "            df2 = series[s2]\n",
    "            \n",
    "            # Create dictionaries for fast lookup\n",
    "            df1_dict = defaultdict(list)\n",
    "            for _, row in df1.iterrows():\n",
    "                df1_dict[row[\"L_sub\"]].append(row.to_dict())\n",
    "            \n",
    "            df2_dict = defaultdict(list)\n",
    "            for _, row in df2.iterrows():\n",
    "                df2_dict[row[\"L_sub\"]].append(row.to_dict())\n",
    "            \n",
    "            # Generate augmented entries\n",
    "            for target_col in self.target_cols:\n",
    "                tf1 = df1[[\"L_sub\", \"R_sub\", f\"Delta_{target_col}\"]].to_dict(\"records\")\n",
    "                tf2 = df2[[\"L_sub\", \"R_sub\", f\"Delta_{target_col}\"]].to_dict(\"records\")\n",
    "                \n",
    "                for entry in tf1:\n",
    "                    l_sub = entry[\"L_sub\"]\n",
    "                    for base in df2_dict.get(l_sub, []):\n",
    "                        r_sub = entry[\"R_sub\"]\n",
    "                        delta = entry[f\"Delta_{target_col}\"]\n",
    "                        new_y = base[f\"L_{target_col}\"] + delta\n",
    "                        smirks_new = l_sub + \">>\" + r_sub\n",
    "                        \n",
    "                        aug_entry = {\n",
    "                            \"CORE\": s1,\n",
    "                            \"L_sub\": l_sub,\n",
    "                            \"R_sub\": r_sub,\n",
    "                            f\"L_{target_col}\": base[f\"L_{target_col}\"],\n",
    "                            f\"R_{target_col}\": new_y,\n",
    "                            f\"Delta_{target_col}\": delta,\n",
    "                            \"AUG\": True,\n",
    "                            \"L_SMILES\": base.get(\"L_SMILES\"),\n",
    "                            \"L_ID\": base.get(\"L_ID\"),\n",
    "                            \"L_sub_ID\": base.get(\"L_sub_ID\"),\n",
    "                            \"R_sub_ID\": self._encode_string(r_sub),\n",
    "                            \"SMIRKS\": smirks_new,\n",
    "                            \"SMIRKS_ID\": self._encode_string(smirks_new),\n",
    "                            \"CORE_ID\": self._encode_string(s1)\n",
    "                        }\n",
    "                        augmented_entries.append(aug_entry)\n",
    "                \n",
    "                for entry in tf2:\n",
    "                    l_sub = entry[\"L_sub\"]\n",
    "                    for base in df1_dict.get(l_sub, []):\n",
    "                        r_sub = entry[\"R_sub\"]\n",
    "                        delta = entry[f\"Delta_{target_col}\"]\n",
    "                        new_y = base[f\"L_{target_col}\"] + delta\n",
    "                        smirks_new = l_sub + \">>\" + r_sub\n",
    "                        \n",
    "                        aug_entry = {\n",
    "                            \"CORE\": s2,\n",
    "                            \"L_sub\": l_sub,\n",
    "                            \"R_sub\": r_sub,\n",
    "                            f\"L_{target_col}\": base[f\"L_{target_col}\"],\n",
    "                            f\"R_{target_col}\": new_y,\n",
    "                            f\"Delta_{target_col}\": delta,\n",
    "                            \"AUG\": True,\n",
    "                            \"L_SMILES\": base.get(\"L_SMILES\"),\n",
    "                            \"L_ID\": base.get(\"L_ID\"),\n",
    "                            \"L_sub_ID\": base.get(\"L_sub_ID\"),\n",
    "                            \"R_sub_ID\": self._encode_string(r_sub),\n",
    "                            \"SMIRKS\": smirks_new,\n",
    "                            \"SMIRKS_ID\": self._encode_string(smirks_new),\n",
    "                            \"CORE_ID\": self._encode_string(s2)\n",
    "                        }\n",
    "                        augmented_entries.append(aug_entry)\n",
    "        \n",
    "        augmented_df = pd.DataFrame(augmented_entries)\n",
    "        \n",
    "        # Mark original data\n",
    "        df_mmps[\"AUG\"] = False\n",
    "        \n",
    "        # Combine original and augmented\n",
    "        combined = pd.concat([df_mmps, augmented_df], ignore_index=True)\n",
    "        \n",
    "        return combined\n",
    "    \n",
    "    def _apply_transformations(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply chemical transformations to generate R_SMILES\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            DataFrame with augmented data\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame with applied transformations\n",
    "        \"\"\"\n",
    "        self._log(\"5) Applying chemical transformations...\")\n",
    "        \n",
    "        df = df.drop_duplicates()\n",
    "        mask = df[\"AUG\"] == True\n",
    "        indices = df[mask].index\n",
    "        \n",
    "        rxn_cache = {}\n",
    "        heavy_cache = {}\n",
    "        core_cache = {}\n",
    "        \n",
    "        new_rows = []\n",
    "        failure_tracker = {\"total\": 0, \"empty_prodsets\": 0}\n",
    "        \n",
    "        for idx in tqdm(indices, desc=\"Transforming\", disable=not self.verbose):\n",
    "            row = df.loc[idx]\n",
    "            l_smiles = row[\"L_SMILES\"]\n",
    "            smirks = row[\"SMIRKS\"]\n",
    "            core = row[\"CORE\"]\n",
    "            \n",
    "            r_smiles_list = self._fast_apply_transformation(\n",
    "                smirks, l_smiles, rxn_cache, core_cache, heavy_cache, core,\n",
    "                failure_tracker\n",
    "            )\n",
    "            \n",
    "            if r_smiles_list:\n",
    "                for r_smiles in r_smiles_list:\n",
    "                    mol = Chem.MolFromSmiles(r_smiles)\n",
    "                    if mol:\n",
    "                        r_id = Chem.InchiToInchiKey(Chem.MolToInchi(mol))\n",
    "                        new_row = row.copy()\n",
    "                        new_row[\"R_SMILES\"] = r_smiles\n",
    "                        new_row[\"R_ID\"] = r_id\n",
    "                        new_rows.append(new_row)\n",
    "        \n",
    "        df_non_aug = df[~mask]\n",
    "        df_aug_expanded = pd.DataFrame(new_rows)\n",
    "        df_final = pd.concat([df_non_aug, df_aug_expanded], ignore_index=True)\n",
    "        \n",
    "        # Report failure rate\n",
    "        total = failure_tracker[\"total\"]\n",
    "        failed = failure_tracker[\"empty_prodsets\"]\n",
    "        if total > 0:\n",
    "            self._log(\n",
    "                f\"⚠️  Empty product sets in {failed}/{total} \"\n",
    "                f\"({100*failed/total:.2f}%) transformations\"\n",
    "            )\n",
    "        \n",
    "        return df_final\n",
    "    \n",
    "    @staticmethod\n",
    "    def _fast_apply_transformation(transformation, l_smiles, rxn_cache, \n",
    "                                   core_cache, heavy_cache, core_smarts,\n",
    "                                   failure_tracker=None):\n",
    "        \"\"\"Apply SMIRKS transformation with caching\"\"\"\n",
    "        if pd.isna(transformation) or pd.isna(l_smiles):\n",
    "            return None\n",
    "        \n",
    "        # Cache reaction\n",
    "        if transformation not in rxn_cache:\n",
    "            try:\n",
    "                rxn = AllChem.ReactionFromSmarts(transformation)\n",
    "                left_smi, right_smi = transformation.split(\">>\")\n",
    "                left_mol = Chem.MolFromSmarts(left_smi)\n",
    "                right_mol = Chem.MolFromSmarts(right_smi)\n",
    "                delta_heavy = right_mol.GetNumHeavyAtoms() - left_mol.GetNumHeavyAtoms()\n",
    "                rxn_cache[transformation] = (rxn, delta_heavy)\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            rxn, delta_heavy = rxn_cache[transformation]\n",
    "        \n",
    "        # Cache molecule\n",
    "        if l_smiles not in heavy_cache:\n",
    "            mol_l = Chem.MolFromSmiles(l_smiles)\n",
    "            if mol_l is None:\n",
    "                return None\n",
    "            n_heavy_l = mol_l.GetNumHeavyAtoms()\n",
    "            heavy_cache[l_smiles] = (mol_l, n_heavy_l)\n",
    "        else:\n",
    "            mol_l, n_heavy_l = heavy_cache[l_smiles]\n",
    "        \n",
    "        # Cache core\n",
    "        if core_smarts not in core_cache:\n",
    "            core_mol = Chem.MolFromSmarts(core_smarts)\n",
    "            if core_mol is None:\n",
    "                return None\n",
    "            core_cache[core_smarts] = core_mol\n",
    "        else:\n",
    "            core_mol = core_cache[core_smarts]\n",
    "        \n",
    "        # Run reaction\n",
    "        try:\n",
    "            products = rxn.RunReactants((mol_l,))\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "        if failure_tracker is not None:\n",
    "            failure_tracker[\"total\"] += 1\n",
    "            if not products:\n",
    "                failure_tracker[\"empty_prodsets\"] += 1\n",
    "        \n",
    "        all_products = []\n",
    "        for prod_set in products:\n",
    "            for prod in prod_set:\n",
    "                if prod is None:\n",
    "                    continue\n",
    "                try:\n",
    "                    if not prod.HasSubstructMatch(core_mol):\n",
    "                        continue\n",
    "                except:\n",
    "                    continue\n",
    "                n_heavy_r = prod.GetNumHeavyAtoms()\n",
    "                if n_heavy_r - n_heavy_l != delta_heavy:\n",
    "                    continue\n",
    "                all_products.append(Chem.MolToSmiles(prod, isomericSmiles=True))\n",
    "        \n",
    "        return all_products if all_products else None\n",
    "    \n",
    "    def _prepare_output(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Prepare final output with proper formatting and statistics\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Processed dataframe\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Final output dataframe\n",
    "        \"\"\"\n",
    "        self._log(\"6) Preparing output...\")\n",
    "        \n",
    "        # Extract L and R entries\n",
    "        output_rows = []\n",
    "        \n",
    "        for target_col in self.target_cols:\n",
    "            l_df = df[[\"L_SMILES\", f\"L_{target_col}\", \"AUG\"]].copy()\n",
    "            l_df.columns = [\"SMILES\", target_col, \"AUG\"]\n",
    "            \n",
    "            r_df = df[[\"R_SMILES\", f\"R_{target_col}\", \"AUG\"]].copy()\n",
    "            r_df.columns = [\"SMILES\", target_col, \"AUG\"]\n",
    "            \n",
    "            combined = pd.concat([l_df, r_df])\n",
    "            output_rows.append(combined)\n",
    "        \n",
    "        # Combine all targets\n",
    "        clean_df = pd.concat(output_rows, axis=0)\n",
    "        \n",
    "        # Remove invalid SMILES\n",
    "        total_before = len(clean_df)\n",
    "        clean_df = clean_df.dropna(subset=[\"SMILES\"])\n",
    "        clean_df = clean_df[clean_df[\"SMILES\"].apply(\n",
    "            lambda x: Chem.MolFromSmiles(x) is not None\n",
    "        )]\n",
    "        total_after = len(clean_df)\n",
    "        \n",
    "        fail_pct = 100 * (total_before - total_after) / total_before\n",
    "        self._log(\n",
    "            f\"Invalid SMILES removed: {total_before - total_after}/{total_before} \"\n",
    "            f\"({fail_pct:.2f}%)\"\n",
    "        )\n",
    "        \n",
    "        # Standardize SMILES\n",
    "        self._log(\"7) Standardizing SMILES...\")\n",
    "        tqdm.pandas(desc=\"Standardizing\", disable=not self.verbose)\n",
    "        clean_df[\"SMILES\"] = clean_df[\"SMILES\"].progress_apply(\n",
    "            lambda x: Chem.MolToSmiles(Chem.MolFromSmiles(x), isomericSmiles=True)\n",
    "        )\n",
    "        \n",
    "        # Generate InChIKeys\n",
    "        self._log(\"8) Generating InChIKeys...\")\n",
    "        tqdm.pandas(desc=\"InChIKeys\", disable=not self.verbose)\n",
    "        clean_df['InChIKey'] = clean_df[\"SMILES\"].progress_apply(self._smiles_to_inchikey)\n",
    "        \n",
    "        # Separate experimental and predicted\n",
    "        clean_df_exp = clean_df[clean_df[\"AUG\"] != True].copy()\n",
    "        clean_df_pred = clean_df[clean_df[\"AUG\"] == True].copy()\n",
    "        \n",
    "        # Aggregate predicted values\n",
    "        output_dfs = []\n",
    "        for target_col in self.target_cols:\n",
    "            grouped_pred = clean_df_pred.groupby(\"InChIKey\")[target_col].agg(\n",
    "                [\"median\", \"std\", \"mean\", \"count\"]\n",
    "            ).reset_index()\n",
    "            grouped_pred.columns = [\n",
    "                \"InChIKey\", \n",
    "                f\"{target_col}_median\",\n",
    "                f\"{target_col}_std\", \n",
    "                f\"{target_col}_mean\",\n",
    "                f\"{target_col}_count\"\n",
    "            ]\n",
    "            grouped_pred[\"AUG\"] = True\n",
    "            \n",
    "            # Filter by std threshold\n",
    "            grouped_pred = grouped_pred[\n",
    "                grouped_pred[f\"{target_col}_std\"] <= self.std_threshold\n",
    "            ]\n",
    "            \n",
    "            # Add SMILES\n",
    "            inchikey_to_smiles = clean_df_pred.drop_duplicates(\"InChIKey\").set_index(\n",
    "                \"InChIKey\"\n",
    "            )[\"SMILES\"].to_dict()\n",
    "            grouped_pred[\"SMILES\"] = grouped_pred[\"InChIKey\"].map(inchikey_to_smiles)\n",
    "            \n",
    "            # Experimental data\n",
    "            exp_out = clean_df_exp[['InChIKey', \"SMILES\", target_col]].copy()\n",
    "            exp_out[f\"{target_col}_median\"] = exp_out[target_col]\n",
    "            exp_out[f\"{target_col}_std\"] = None\n",
    "            exp_out[f\"{target_col}_mean\"] = exp_out[target_col]\n",
    "            exp_out[f\"{target_col}_count\"] = 1\n",
    "            exp_out[\"AUG\"] = False\n",
    "            exp_out = exp_out.drop(columns=[target_col])\n",
    "            \n",
    "            # Remove predicted entries that exist in experimental\n",
    "            known_keys = set(exp_out['InChIKey'])\n",
    "            grouped_pred = grouped_pred[~grouped_pred['InChIKey'].isin(known_keys)]\n",
    "            \n",
    "            # Combine\n",
    "            combined = pd.concat([\n",
    "                exp_out.drop_duplicates(\"InChIKey\"),\n",
    "                grouped_pred.drop_duplicates(\"InChIKey\")\n",
    "            ], ignore_index=True)\n",
    "            \n",
    "            output_dfs.append(combined)\n",
    "        \n",
    "        # Merge all targets\n",
    "        final_df = output_dfs[0]\n",
    "        for df_target in output_dfs[1:]:\n",
    "            final_df = final_df.merge(df_target, on=[\"InChIKey\", \"SMILES\", \"AUG\"], how=\"outer\")\n",
    "        \n",
    "        # Calculate statistics\n",
    "        self.statistics = {\n",
    "            \"n_original\": len(clean_df_exp.drop_duplicates(\"InChIKey\")),\n",
    "            \"n_augmented\": len(grouped_pred.drop_duplicates(\"InChIKey\")),\n",
    "            \"n_total\": len(final_df),\n",
    "            \"augmentation_ratio\": len(grouped_pred) / len(clean_df_exp) if len(clean_df_exp) > 0 else 0\n",
    "        }\n",
    "        \n",
    "        self._log(f\"✅ Augmentation complete!\")\n",
    "        self._log(f\"   Original compounds: {self.statistics['n_original']}\")\n",
    "        self._log(f\"   Augmented compounds: {self.statistics['n_augmented']}\")\n",
    "        self._log(f\"   Total compounds: {self.statistics['n_total']}\")\n",
    "        self._log(f\"   Augmentation ratio: {self.statistics['augmentation_ratio']:.2f}x\")\n",
    "        \n",
    "        return final_df\n",
    "    \n",
    "    def run(self, output_path: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Run the complete augmentation pipeline\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        output_path : str, optional\n",
    "            Path to save the output CSV/parquet file\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Augmented dataset with statistics\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create temporary output file\n",
    "        with tempfile.NamedTemporaryFile(\n",
    "            mode='w', suffix='.csv', delete=False\n",
    "        ) as tmp_file:\n",
    "            tmp_output = tmp_file.name\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Fragment and generate MMPs\n",
    "            df_mmps = self._fragment_molecules(tmp_output)\n",
    "            \n",
    "            # Step 2: Augment data\n",
    "            df_augmented = self._augment_data(df_mmps)\n",
    "            \n",
    "            # Step 3: Apply transformations\n",
    "            df_transformed = self._apply_transformations(df_augmented)\n",
    "            \n",
    "            # Step 4: Prepare output\n",
    "            self.augmented_df = self._prepare_output(df_transformed)\n",
    "            \n",
    "            # Save if path provided\n",
    "            if output_path:\n",
    "                if output_path.endswith('.parquet'):\n",
    "                    self.augmented_df.to_parquet(output_path, index=False)\n",
    "                else:\n",
    "                    self.augmented_df.to_csv(output_path, index=False)\n",
    "                self._log(f\"Output saved to: {output_path}\")\n",
    "            \n",
    "        finally:\n",
    "            # Cleanup temporary file\n",
    "            if os.path.exists(tmp_output):\n",
    "                os.unlink(tmp_output)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        self._log(f\"⏱️  Total time: {elapsed:.2f} seconds\")\n",
    "        \n",
    "        # Get InChIKeys from augmented_df\n",
    "        augmented_inchikeys = set(self.augmented_df['InChIKey'].dropna())\n",
    "\n",
    "        # Get InChIKeys from original\n",
    "        self.df_original['InChIKey'] = self.df_original[self.smiles_col].apply(self._smiles_to_inchikey)\n",
    "        original_inchikeys = set(self.df_original['InChIKey'].dropna())\n",
    "\n",
    "        # Find missing compounds\n",
    "        missing_inchikeys = original_inchikeys - augmented_inchikeys\n",
    "\n",
    "        if len(missing_inchikeys) > 0:\n",
    "            self._log(f\"Adding {len(missing_inchikeys)} missing original compounds...\")\n",
    "\n",
    "            # Get missing compounds from original\n",
    "            missing_df = self.df_original[self.df_original['InChIKey'].isin(missing_inchikeys)].copy()\n",
    "\n",
    "            # Format to match augmented_df structure\n",
    "            missing_formatted = pd.DataFrame()\n",
    "            missing_formatted['InChIKey'] = missing_df['InChIKey']\n",
    "            missing_formatted['SMILES'] = missing_df[self.smiles_col]\n",
    "            missing_formatted['AUG'] = False\n",
    "\n",
    "            # Add target columns\n",
    "            for target_col in self.target_cols:\n",
    "                missing_formatted[f'{target_col}_median'] = missing_df[target_col]\n",
    "                missing_formatted[f'{target_col}_mean'] = missing_df[target_col]\n",
    "                missing_formatted[f'{target_col}_std'] = None\n",
    "                missing_formatted[f'{target_col}_count'] = 1\n",
    "\n",
    "            # Concatenate\n",
    "            self.augmented_df = pd.concat([self.augmented_df, missing_formatted], ignore_index=True)\n",
    "\n",
    "            # Update statistics\n",
    "            self.statistics['n_original'] = len(original_inchikeys)        \n",
    "        \n",
    "        return self.augmented_df\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Return augmentation statistics\"\"\"\n",
    "        return self.statistics\n",
    "    \n",
    "    def get_experimental_only(self) -> pd.DataFrame:\n",
    "        \"\"\"Return only experimental compounds\"\"\"\n",
    "        if self.augmented_df is None:\n",
    "            raise ValueError(\"Run augmentation first using .run()\")\n",
    "        return self.augmented_df[self.augmented_df[\"AUG\"] == False].copy()\n",
    "    \n",
    "    def get_augmented_only(self) -> pd.DataFrame:\n",
    "        \"\"\"Return only augmented compounds\"\"\"\n",
    "        if self.augmented_df is None:\n",
    "            raise ValueError(\"Run augmentation first using .run()\")\n",
    "        return self.augmented_df[self.augmented_df[\"AUG\"] == True].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b867b-aabe-4024-bf33-257bcbc8757a",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "238f6091-f948-4df1-81b2-1d575c7ee680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/exp_subset/LogBB.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "460dae16-8da8-4c5a-ba49-869e4d2df6ec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of input dataset:  540\n",
      "[MMPAugmentor] 0) Generating molecular fragments...\n",
      "[MMPAugmentor] 1) Indexing fragments and generating MMPs...\n",
      "[MMPAugmentor] 2) Canonicalizing SMIRKS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:11] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:12] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:12] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:12] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:12] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:12] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:12] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:12] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:12] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:12] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:12] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:12] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:12] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:13] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:13] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:13] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:13] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:13] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:13] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:13] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:13] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:13] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:13] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:13] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:13] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:13] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:13] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:14] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:14] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:14] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:14] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:14] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:14] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:14] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:14] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:14] WARNING: not removing hydrogen atom with dummy atom neighbors\n",
      "[11:38:14] WARNING: not removing hydrogen atom with dummy atom neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MMPAugmentor] 3) Computing pairwise scaffold correlations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing correlations:  77%|███████▋  | 1330167/1734453 [00:18<00:07, 54252.59it/s]/var/tmp/ipykernel_189535/571203446.py:304: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr = pearsonr(y1, y2)[0]\n",
      "Computing correlations:  77%|███████▋  | 1343413/1734453 [00:18<00:06, 57864.89it/s]/var/tmp/ipykernel_189535/571203446.py:304: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr = pearsonr(y1, y2)[0]\n",
      "Computing correlations: 100%|██████████| 1734453/1734453 [00:24<00:00, 72048.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MMPAugmentor] 4) Found 43 valid scaffold pairs. Generating augmented data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting: 100%|██████████| 43/43 [00:03<00:00, 12.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MMPAugmentor] 5) Applying chemical transformations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming: 100%|██████████| 4921/4921 [00:07<00:00, 675.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MMPAugmentor] ⚠️  Empty product sets in 35/4921 (0.71%) transformations\n",
      "[MMPAugmentor] 6) Preparing output...\n",
      "[MMPAugmentor] Invalid SMILES removed: 0/121540 (0.00%)\n",
      "[MMPAugmentor] 7) Standardizing SMILES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standardizing: 100%|██████████| 121540/121540 [00:22<00:00, 5370.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MMPAugmentor] 8) Generating InChIKeys...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "InChIKeys: 100%|██████████| 121540/121540 [00:54<00:00, 2250.62it/s]\n",
      "/var/tmp/ipykernel_189535/571203446.py:635: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MMPAugmentor] ✅ Augmentation complete!\n",
      "[MMPAugmentor]    Original compounds: 459\n",
      "[MMPAugmentor]    Augmented compounds: 501\n",
      "[MMPAugmentor]    Total compounds: 960\n",
      "[MMPAugmentor]    Augmentation ratio: 0.00x\n",
      "[MMPAugmentor] Output saved to: augmented_data.csv\n",
      "[MMPAugmentor] ⏱️  Total time: 179.32 seconds\n",
      "[MMPAugmentor] Adding 81 missing original compounds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_189535/571203446.py:744: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.augmented_df = pd.concat([self.augmented_df, missing_formatted], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of input dataset: \", len(df))\n",
    "\n",
    "# Create augmentor with relaxed thresholds for the example\n",
    "augmentor = MMPDataAugmentor(\n",
    "    df=df,\n",
    "    smiles_col=\"SMILES\",\n",
    "    target_cols=[\"LogBB\"],\n",
    "    max_heavy=15,          # Allow smaller changes\n",
    "    max_ratio=0.4,        # More permissive ratio\n",
    "    min_common=3,         # Lower threshold for common MMPs\n",
    "    pearson_thresh=0.3,   # Lower correlation threshold\n",
    "    crmsd_thresh=1.,     # Higher cRMSD tolerance\n",
    "    std_threshold=1.0,    # Higher std tolerance\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run augmentation\n",
    "augmented_df = augmentor.run(output_path=\"augmented_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "642a8448-22fa-4230-bf9b-6444c6574685",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InChIKey</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>LogBB_median</th>\n",
       "      <th>LogBB_std</th>\n",
       "      <th>LogBB_mean</th>\n",
       "      <th>LogBB_count</th>\n",
       "      <th>AUG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DOMXUEMWDBAQBQ-UHFFFAOYSA-N</td>\n",
       "      <td>CN(CC=CC#CC(C)(C)C)Cc1cccc2ccccc12</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AQHHHDLHHXJYJD-UHFFFAOYSA-N</td>\n",
       "      <td>CC(C)NCC(O)COc1cccc2ccccc12</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NJXRHIKVLFLBME-UHFFFAOYSA-N</td>\n",
       "      <td>O=C(NC1C2CC3CC1CC(O)(C3)C2)c1sc(OCCO)nc1C1CC1</td>\n",
       "      <td>-0.849485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.849485</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACMDXKZEHRFMAQ-UHFFFAOYSA-N</td>\n",
       "      <td>COCCOc1nc(C2CC2)c(C(=O)NC2C3CC4CC2CC(O)(C4)C3)s1</td>\n",
       "      <td>-0.071334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.071334</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RPLJDCCDRTXZTG-UHFFFAOYSA-N</td>\n",
       "      <td>O=C(NC1C2CC3CC1CC(O)(C3)C2)c1sc(OC2CCOCC2)nc1C...</td>\n",
       "      <td>-0.071334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.071334</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>VOKSWYLNZZRQPF-UHFFFAOYSA-N</td>\n",
       "      <td>CC(C)=CCN1CCC2(C)C(C)C1Cc1ccc(O)cc21</td>\n",
       "      <td>0.543750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.543750</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>ZULQKQGLUORZDC-UHFFFAOYSA-N</td>\n",
       "      <td>N#Cc1cc(ccc1)N1CCN(CCN2CCC(CC2)C(F)(F)F)C1=O</td>\n",
       "      <td>0.806180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.806180</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>LAVIXOKYHRWFDZ-UHFFFAOYSA-N</td>\n",
       "      <td>CC1CCC(C)N1c1ccc([n][n]1)-c1c[n]cc2ccccc21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>PZAIVSQWZAHADG-UHFFFAOYSA-N</td>\n",
       "      <td>CC(C)(C(=O)Nc1ccc(c(Cl)c1)N1CCC2(CN(CC2)CC2CC2...</td>\n",
       "      <td>0.361728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.361728</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>TVMNFBNOOMKBOO-UHFFFAOYSA-N</td>\n",
       "      <td>C[n]1c([n][n]c1C1CCOCC1)SCCCN1CCc2cc(ccc2CC1)-...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1041 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         InChIKey  \\\n",
       "0     DOMXUEMWDBAQBQ-UHFFFAOYSA-N   \n",
       "1     AQHHHDLHHXJYJD-UHFFFAOYSA-N   \n",
       "2     NJXRHIKVLFLBME-UHFFFAOYSA-N   \n",
       "3     ACMDXKZEHRFMAQ-UHFFFAOYSA-N   \n",
       "4     RPLJDCCDRTXZTG-UHFFFAOYSA-N   \n",
       "...                           ...   \n",
       "1036  VOKSWYLNZZRQPF-UHFFFAOYSA-N   \n",
       "1037  ZULQKQGLUORZDC-UHFFFAOYSA-N   \n",
       "1038  LAVIXOKYHRWFDZ-UHFFFAOYSA-N   \n",
       "1039  PZAIVSQWZAHADG-UHFFFAOYSA-N   \n",
       "1040  TVMNFBNOOMKBOO-UHFFFAOYSA-N   \n",
       "\n",
       "                                                 SMILES  LogBB_median  \\\n",
       "0                    CN(CC=CC#CC(C)(C)C)Cc1cccc2ccccc12      0.090000   \n",
       "1                           CC(C)NCC(O)COc1cccc2ccccc12      0.640000   \n",
       "2         O=C(NC1C2CC3CC1CC(O)(C3)C2)c1sc(OCCO)nc1C1CC1     -0.849485   \n",
       "3      COCCOc1nc(C2CC2)c(C(=O)NC2C3CC4CC2CC(O)(C4)C3)s1     -0.071334   \n",
       "4     O=C(NC1C2CC3CC1CC(O)(C3)C2)c1sc(OC2CCOCC2)nc1C...     -0.071334   \n",
       "...                                                 ...           ...   \n",
       "1036               CC(C)=CCN1CCC2(C)C(C)C1Cc1ccc(O)cc21      0.543750   \n",
       "1037       N#Cc1cc(ccc1)N1CCN(CCN2CCC(CC2)C(F)(F)F)C1=O      0.806180   \n",
       "1038         CC1CCC(C)N1c1ccc([n][n]1)-c1c[n]cc2ccccc21      0.000000   \n",
       "1039  CC(C)(C(=O)Nc1ccc(c(Cl)c1)N1CCC2(CN(CC2)CC2CC2...      0.361728   \n",
       "1040  C[n]1c([n][n]c1C1CCOCC1)SCCCN1CCc2cc(ccc2CC1)-...     -1.000000   \n",
       "\n",
       "      LogBB_std  LogBB_mean  LogBB_count    AUG  \n",
       "0           NaN    0.090000            1  False  \n",
       "1           NaN    0.640000            1  False  \n",
       "2           NaN   -0.849485            1  False  \n",
       "3           NaN   -0.071334            1  False  \n",
       "4           NaN   -0.071334            1  False  \n",
       "...         ...         ...          ...    ...  \n",
       "1036        NaN    0.543750            1  False  \n",
       "1037        NaN    0.806180            1  False  \n",
       "1038        NaN    0.000000            1  False  \n",
       "1039        NaN    0.361728            1  False  \n",
       "1040        NaN   -1.000000            1  False  \n",
       "\n",
       "[1041 rows x 7 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bce4282-7f9d-4830-bb98-8ac7ec4ad457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-chemprop_MTL-chemprop_mtl",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "chemprop_MTL",
   "language": "python",
   "name": "conda-env-chemprop_MTL-chemprop_mtl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
